# ğŸ¤– Client LLM â€” Recettes & Quran

## ğŸ“˜ AperÃ§u

Ce module `client_llm` implÃ©mente un **client intelligent** capable de communiquer :
- avec un **LLM local** (via `llama-cli` et le modÃ¨le *Mistral 7B*),
- avec un **backend distant FastAPI** (simulation locale),
- ou avec un **serveur LLM global** basÃ© sur RAG + Redis (centralisÃ©).

Lâ€™objectif est de fournir un systÃ¨me **hybride** et **rÃ©silient**, capable de fonctionner :
- ğŸ”¹ en **mode hors ligne (offline)** â€” via SQLite et le LLM local,
- ğŸ”¹ en **mode en ligne (online)** â€” via un backend FastAPI local,
- ğŸ”¹ en **mode serveur (server)** â€” via une API centralisÃ©e (RAG + cache Redis).

---

## ğŸ§± FonctionnalitÃ©s principales

### ğŸ³ Gestion des Recettes
- Base locale `recipes.db` initialisÃ©e Ã  partir de `data/recipes.json`
- Recherche rapide par ingrÃ©dient
- GÃ©nÃ©ration de suggestions via le LLM local ou distant

### ğŸ“– Gestion du Coran
- Base locale `surrah.db` crÃ©Ã©e depuis `data/quran_complete.json`
- Recherche par nom de sourate (arabe, franÃ§ais, anglais)
- Lecture de liens audio associÃ©s aux sourates

### ğŸ§  Modes de fonctionnement
| Mode | Source | Description |
|------|---------|-------------|
| **offline** | Local DB + LLM local | Fonctionne sans Internet |
| **online** | Backend FastAPI local | RequÃªtes simulÃ©es au serveur local |
| **server** | Serveur LLM RAG | Connexion Ã  lâ€™API centralisÃ©e avec cache Redis |

---
## âœ”ï¸ Avantages du Projet

Ce projet offre une architecture flexible, performante et adaptÃ©e Ã  diffÃ©rents environnements dâ€™exÃ©cution.

### ğŸ”¹ 1. Multi-mode de fonctionnement
- **Offline** : fonctionne entiÃ¨rement sans connexion rÃ©seau grÃ¢ce aux bases locales SQLite et au modÃ¨le LLM quantifiÃ© (Mistral).
- **Online** : interagit avec le backend FastAPI pour rÃ©cupÃ©rer ou synchroniser les donnÃ©es.
- **Server** : se connecte au serveur LLM global (RAG + Redis) pour bÃ©nÃ©ficier du cache et de la gÃ©nÃ©ration contextuelle.

### ğŸ”¹ 2. Architecture modulaire et extensible
- SÃ©paration claire entre les couches :
    - `local_db.py` : gestion des bases locales (Recettes + Coran)
    - `llm_client.py` : communication avec les modÃ¨les IA (local/distant)
    - `backend_local.py` : API FastAPI simulÃ©e pour test local
- Facile Ã  Ã©tendre avec dâ€™autres domaines (ex. Qissas, Hadith, etc.).

### ğŸ”¹ 3. IntÃ©gration hybride (LLM + donnÃ©es structurÃ©es)
- Combine la **gÃ©nÃ©ration par IA** (LLM Mistral) avec la **recherche locale rapide** (SQLite).
- Permet des suggestions intelligentes tout en gardant la cohÃ©rence des donnÃ©es.

### ğŸ”¹ 4. Performances et robustesse
- Cache intelligent pour rÃ©duire le coÃ»t des appels au LLM.
- Chargement rapide des donnÃ©es locales (moins de 200 ms).
- InfÃ©rence locale optimisÃ©e (modÃ¨le quantifiÃ© ~300 Mo).

### ğŸ”¹ 5. FacilitÃ© de test et de dÃ©ploiement
- Compatible **terminal / mobile / backend**.
- Testable via `python3 -m client_llm.main_flow` sans dÃ©pendances externes lourdes.
- IntÃ©gration directe dans Android Studio pour dÃ©veloppement mobile.

---

ğŸš€ **En rÃ©sumÃ© :**
> `client_llm` agit comme un cerveau local intelligent, capable dâ€™utiliser un modÃ¨le IA en mode hors-ligne, de communiquer avec un serveur distant, et de sâ€™intÃ©grer dans un systÃ¨me RAG complet.
---

## ğŸ—‚ï¸ Structure du Projet


```text
client_llm/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ backend.py              # Backend FastAPI simulant un serveur distant
â”œâ”€â”€ llm_client.py           # Gestion du LLM local / distant / serveur global
â”œâ”€â”€ main_flow.py            # Flux principal (choix offline / online / server)
â”œâ”€â”€ local_db.py             # Initialisation et recherche dans les DB locales (Recettes & Quran)
â”œâ”€â”€ recipe_db               # Base locale SQLite des recettes
â”œâ”€â”€ surrah_db               # Base locale SQLite des sourates
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ recipes.json        # DonnÃ©es JSON des recettes
â”‚   â”œâ”€â”€ quran_complete.json # DonnÃ©es JSON complÃ¨tes du Coran
â”‚   
â””â”€â”€ README.md

``` 
## âš™ï¸ Installation

### ğŸ§© PrÃ©requis

- Python â‰¥ 3.9
- FastAPI + Uvicorn
- ModÃ¨le Mistral tÃ©lÃ©chargÃ© pour `llama-cli`
- (Optionnel) Redis si tu veux tester le cache serveur global

### ğŸ§° Installation des dÃ©pendances
```bash
pip install fastapi uvicorn requests pydantic

ğŸ”¸ Mode Offline
python3 -m client_llm.main_flow
IngrÃ©dient Ã  rechercher : tomate
Mode (offline/online/server) : offline
RÃ©sultats locaux :
- Quiche tomates et Ã©pinards

ğŸ”¸ Mode Online (Backend Local FastAPI)
python3 -m client_llm.main_flow
Mode (offline/online/server) : online
Aucune recette sur le serveur. GÃ©nÃ©ration LLM distant...
Recette gÃ©nÃ©rÃ©e par LLM distant pour 'Recette avec l'ingrÃ©dient tomate' (simulation)

ğŸ”¸ Mode Server (RAG + Redis)
python3 -m client_llm.main_flow
Mode (offline/online/server) : server
[Cache Redis] La premiÃ¨re sourate du Coran est Al-Fatiha.
