{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "030c7eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/Desktop/AGH_Data_Agency_Holding_SA/Projet_LLM_Server/Server_LLM/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "import json\n",
    "import requests\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2c074a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================\n",
    "# configurations\n",
    "#========================================\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "API_URL = \"http://localhost:11434/api/generate\"\n",
    "EMBEDDING_MODEL_NAME = \"multi-qa-mpnet-base-dot-v1\"\n",
    "DATA = torch.load(\"/home/ahmed/Desktop/AGH_Data_Agency_Holding_SA/Projet_LLM_Server/data/meal_embeddings.pt\")\n",
    "EMBEDDINGS_MEAL = DATA[\"embeddings\"]\n",
    "TEXTS_MEAL = DATA[\"texts\"]\n",
    "SIMILARITY_THRESHOLD = 0.85\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f715b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources_person(\n",
    "    query: str,\n",
    "    embeddings: torch.tensor,\n",
    "    model_name: str = EMBEDDING_MODEL_NAME,\n",
    "    n_resources_to_return: int = 5,\n",
    "):\n",
    "    model = SentenceTransformer(model_name, device=\"cpu\")\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "\n",
    "    scores, indices = torch.topk(dot_scores, k=n_resources_to_return)\n",
    "\n",
    "    return indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter_person(query: str, context_items: list[str]) -> str:\n",
    "    context = \"- \" + \"\\n- \".join(context_items)\n",
    "    \n",
    "    base_prompt = f\"\"\"\n",
    "    You are an expert cooking assistant.\n",
    "\n",
    "    Below are recipes from the database:\n",
    "    {context}\n",
    "\n",
    "    User question:\n",
    "    {query}\n",
    "\n",
    "    Answer using the database information when possible.\n",
    "    If there is no relevant recipe, give a general cooking answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # print(f\"the context is :{base_prompt}\")\n",
    "    return base_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_response(ollama_model_name, prompt):\n",
    "    data = {\n",
    "        \"model\": ollama_model_name,\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "    \n",
    "    response_text = \"\"\n",
    "    response = requests.post(API_URL, json=data, stream=True)\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            text = json.loads(line)[\"response\"]\n",
    "            response_text += text  \n",
    "    \n",
    "    return response_text  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automate_asking(query, ollama_model_name):\n",
    "    indices = retrieve_relevant_resources_person(query=query, embeddings=EMBEDDINGS_MEAL)\n",
    "    context_items = [TEXTS_MEAL[i] for i in indices]\n",
    "\n",
    "    prompt = prompt_formatter_person(query=query, context_items=context_items)\n",
    "    return llm_response(ollama_model_name, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad12927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str):\n",
    "    sim_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=\"cpu\")\n",
    "    return sim_model.encode(text, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a619251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_question(query_emb):\n",
    "    for key in r.keys():\n",
    "        raw = r.get(key)\n",
    "        if not raw:\n",
    "            continue  \n",
    "\n",
    "        try:\n",
    "            cached_data = json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            continue  \n",
    "\n",
    "        cached_emb = torch.tensor(cached_data[\"embedding\"])\n",
    "        similarity = util.cos_sim(query_emb, cached_emb).item()\n",
    "        if similarity >= SIMILARITY_THRESHOLD:\n",
    "            return key.decode(), cached_data[\"response\"]\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d4853ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm_with_redis_smart(question):\n",
    "    query_emb = get_embedding(question)\n",
    "    \n",
    "    similar_key, cached_response = find_similar_question(query_emb)\n",
    "    \n",
    "    if cached_response:\n",
    "        print(\"Using cached response for a similar question!\")\n",
    "        from_cache = True\n",
    "        response_text = cached_response\n",
    "    else:\n",
    "        response_text = automate_asking(question, ollama_model_name=\"llama3.2\")\n",
    "        data_to_cache = {\n",
    "            \"embedding\": query_emb.tolist(), \n",
    "            \"response\": response_text\n",
    "        }\n",
    "        r.set(question, json.dumps(data_to_cache))\n",
    "        from_cache = False\n",
    "    \n",
    "    return response_text, from_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLM Resposne is :\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A delicious dessert! According to our database, I have just the recipe for you: Banana Pancakes!\n",
       "\n",
       "Here's how to make it:\n",
       "\n",
       "Ingredients:\n",
       "- 1 large banana\n",
       "- 2 medium eggs\n",
       "- A pinch of baking powder\n",
       "- A sprinkling of vanilla extract\n",
       "- 1 tsp oil\n",
       "- 25g pecan nuts\n",
       "- 125g raspberries\n",
       "\n",
       "Instructions:\n",
       "1. Mash the banana with a fork until it resembles a thick purée.\n",
       "2. Stir in the eggs, baking powder, and vanilla extract.\n",
       "3. Heat a large non-stick frying pan or pancake pan over medium heat and brush with half the oil.\n",
       "4. Using half the batter, spoon two pancakes into the pan, cook for 1-2 mins each side, then tip onto a plate.\n",
       "5. Repeat the process with the remaining oil and batter.\n",
       "6. Top the pancakes with pecans and raspberries.\n",
       "\n",
       "Enjoy your Banana Pancakes!\n",
       "\n",
       "(Note: I couldn't find any additional information about this recipe in our database. If you'd like more guidance or variations, feel free to ask!)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache: False\n",
      "Temps: 204.81155562400818\n",
      "\n",
      "\n",
      "\n",
      "############################################################\n",
      "\n",
      "\n",
      "\n",
      "Using cached response for a similar question!\n",
      "The LLM Resposne is :\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A delicious dessert! According to our database, I have just the recipe for you: Banana Pancakes!\n",
       "\n",
       "Here's how to make it:\n",
       "\n",
       "Ingredients:\n",
       "- 1 large banana\n",
       "- 2 medium eggs\n",
       "- A pinch of baking powder\n",
       "- A sprinkling of vanilla extract\n",
       "- 1 tsp oil\n",
       "- 25g pecan nuts\n",
       "- 125g raspberries\n",
       "\n",
       "Instructions:\n",
       "1. Mash the banana with a fork until it resembles a thick purée.\n",
       "2. Stir in the eggs, baking powder, and vanilla extract.\n",
       "3. Heat a large non-stick frying pan or pancake pan over medium heat and brush with half the oil.\n",
       "4. Using half the batter, spoon two pancakes into the pan, cook for 1-2 mins each side, then tip onto a plate.\n",
       "5. Repeat the process with the remaining oil and batter.\n",
       "6. Top the pancakes with pecans and raspberries.\n",
       "\n",
       "Enjoy your Banana Pancakes!\n",
       "\n",
       "(Note: I couldn't find any additional information about this recipe in our database. If you'd like more guidance or variations, feel free to ask!)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache: True\n",
      "Temps: 5.0426716804504395\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "res, from_cache = ask_llm_with_redis_smart(\"can you tell me a Dessert recette Banana Pancakes\")\n",
    "print(\"The LLM Resposne is :\\n\")\n",
    "display(Markdown(res))\n",
    "print(\"Cache:\", from_cache)\n",
    "print(\"Temps:\", time.time() - start)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\n############################################################\\n\\n\\n\")\n",
    "\n",
    "start = time.time()\n",
    "res, from_cache = ask_llm_with_redis_smart(\"i want a Dessert recette Banana Pancakes\")\n",
    "print(\"The LLM Resposne is :\\n\")\n",
    "display(Markdown(res))\n",
    "print(\"Cache:\", from_cache)\n",
    "print(\"Temps:\", time.time() - start)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Server_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
